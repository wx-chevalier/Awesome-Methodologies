# AI CheatSheet | AI

æ‰€è°“ AIï¼Œå³èƒ½å¤Ÿæ„ŸçŸ¥ï¼ˆAware/Senseï¼‰ï¼Œç„¶ååšå‡ºå†³ç­–ï¼ˆDecisionï¼‰ï¼Œå¹¶ä¸”å¯¹äºç¯å¢ƒåšå‡ºååº”ï¼ˆActï¼‰ã€‚

äººå·¥æ™ºèƒ½ï¼Œä¸æœºå™¨å­¦ä¹ çš„æ€ç»´æ¨¡å¼ï¼Œä¸åŒäºè½¯ä»¶å·¥ç¨‹ä¸­ä»¥é€»è¾‘ä¸æ•°å­¦æ€ç»´å»æ€è€ƒï¼Œä»¥æ–­è¨€æ¥è¯æ˜ç¨‹åºå„å±æ€§çš„æ­£ç¡®æ€§ï¼Œå¹¶ä¸”æœ€ç»ˆå¾—åˆ°ç¡®å®šæ€§äº§å“çš„å¼€å‘æ€ç»´æ¨¡å¼ï¼›å®ƒçš„å…³æ³¨ç‚¹ä»æ•°å­¦ç§‘å­¦è½¬ç§»åˆ°è‡ªç„¶ç§‘å­¦ï¼Œè§‚å¯Ÿä¸ç¡®å®šçš„æœªçŸ¥ä¸–ç•Œï¼Œå¼€å±•å®éªŒï¼Œå¹¶ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ è€Œéé€»è¾‘æ¥åˆ†æå®éªŒç»“æœã€‚

## æ„ä¹‰ä¸åº”ç”¨

é¦–å…ˆï¼Œå®ƒä¼šä¸ºæ‚¨æä¾›ä¸€ä¸ªå¯ç¼©çŸ­ç¼–ç¨‹æ—¶é—´çš„å·¥å…·ã€‚å‡è®¾æˆ‘æƒ³ç¼–å†™ä¸€ä¸ªç¨‹åºæ¥çº æ­£æ‹¼å†™é”™è¯¯ã€‚æˆ‘å¯ä»¥é€šè¿‡å¤§é‡ç¤ºä¾‹å’Œç»éªŒæ³•åˆ™ ï¼ˆä¾‹å¦‚ï¼ŒI ä½äº E ä¹‹å‰ï¼Œä½†å‡ºç°åœ¨ C ä¹‹åæ—¶ä¾‹å¤–ï¼‰ï¼Œå–å¾—ä¸€å®šçš„è¿›å±•ï¼Œç„¶åç»è¿‡æ•°å‘¨çš„åŠªåŠ›ï¼Œç¼–å†™å‡ºä¸€ä¸ªåˆç†çš„ç¨‹åºã€‚

å…¶æ¬¡ï¼Œå€ŸåŠ©æœºå™¨å­¦ä¹ ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰è‡ªå·±çš„äº§å“ï¼Œä½¿å…¶æ›´é€‚åˆç‰¹å®šçš„ç”¨æˆ·ç¾¤ä½“ã€‚å‡è®¾æˆ‘æ‰‹åŠ¨ç¼–å†™äº†ä¸€ä¸ªè‹±æ–‡æ‹¼å†™çº æ­£ç¨‹åºï¼Œè¿™ä¸ªç¨‹åºå¾ˆæˆåŠŸï¼Œå› æ­¤æˆ‘æ‰“ç®— é’ˆå¯¹ 100 ç§æœ€å¸¸ç”¨è¯­è¨€æä¾›ç›¸åº”çš„ç‰ˆæœ¬ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ¯ç§è¯­è¨€ç‰ˆæœ¬å‡ ä¹éƒ½éœ€è¦ä»å¤´å¼€å§‹ï¼Œè¿™å°†éœ€è¦ä»˜å‡ºæ•°å¹´çš„åŠªåŠ›ã€‚ä½†å¦‚æœæˆ‘ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ„å»ºè¯¥ç¨‹åºï¼Œ ç„¶åè¿ç§»åˆ°å…¶ä»–è¯­è¨€ï¼ŒåŸºæœ¬ä¸Šå°±ç›¸å½“äºï¼Œæˆ‘åªéœ€æ”¶é›†è¯¥ç‰¹å®šè¯­è¨€çš„æ•°æ®ï¼Œ å¹¶å°†è¿™äº›æ•°æ®æä¾›ç»™å®Œå…¨ä¸€æ ·çš„æœºå™¨å­¦ä¹ æ¨¡å‹å³å¯ã€‚

ç¬¬ä¸‰ï¼Œå€ŸåŠ©æœºå™¨å­¦ä¹ ï¼Œæ‚¨å¯ä»¥è§£å†³è‡ªå·±ä½œä¸ºç¼–ç¨‹äººå‘˜ä¸çŸ¥é“å¦‚ä½•ç”¨äººå·¥æ–¹æ³•è§£å†³çš„é—®é¢˜ã€‚ä½œä¸ºäººç±»ï¼Œæˆ‘å¯ä»¥è®¤å‡ºæœ‹å‹çš„é¢å­”ï¼Œ ç†è§£ä»–ä»¬æ‰€è¯´çš„è¯ï¼Œä½†æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨æ½œæ„è¯†ä¸‹å®Œæˆçš„ï¼Œ å¦‚æœè®©æˆ‘ç¼–å†™ä¸€ä¸ªç¨‹åºæ¥åšè¿™äº›äº‹ï¼Œæˆ‘ä¼šå®Œå…¨ä¸çŸ¥æ‰€æªã€‚ä½†æ˜¯ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯¹æ­¤å´å¾ˆæ“…é•¿ï¼›æˆ‘ä¸éœ€è¦å‘Šè¯‰ç®—æ³•åº”è¯¥æ€ä¹ˆåšï¼Œåªéœ€å‘å…¶å±•ç¤ºå¤§é‡æ ·æœ¬ï¼Œ é—®é¢˜å°±å¯ä»¥è¿åˆƒè€Œè§£ã€‚

## å‘å±•ä¸å˜è¿

äººå·¥æ™ºèƒ½å‘å±•æœ‰ä¸‰ä¸ªé˜¶æ®µï¼šè®¡ç®—æ™ºèƒ½ã€æ„ŸçŸ¥æ™ºèƒ½å’Œè®¤çŸ¥æ™ºèƒ½ã€‚

ç¬¬ä¸€é˜¶æ®µçš„è®¡ç®—æ™ºèƒ½å³å¿«é€Ÿè®¡ç®—å’Œè®°å¿†å­˜å‚¨ï¼Œåƒæœºå™¨äººæˆ˜èƒœå›´æ£‹å¤§å¸ˆï¼Œé çš„å°±æ˜¯è¶…å¼ºçš„è®°å¿†èƒ½åŠ›å’Œè¿ç®—é€Ÿåº¦ã€‚äººè„‘çš„é€»è¾‘èƒ½åŠ›å†å¼ºå¤§ï¼Œä¹Ÿæ•Œä¸è¿‡äººå·¥æ™ºèƒ½æ¯å¤©å’Œè‡ªå·±ä¸‹å‡ ç™¾ç›˜æ£‹ï¼Œé€šè¿‡å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å¯¹åå‡ æ­¥åçš„ç»“æœåšå‡ºé¢„æµ‹ï¼Œä»è¿™ä¸€è§’åº¦æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½å¤šæ¬¡æˆ˜è´¥ä¸–ç•Œçº§å›´æ£‹é€‰æ‰‹ï¼Œè¶³ä»¥è¯æ˜è¿™ä¸€é¢†åŸŸå‘å±•ä¹‹æˆç†Ÿã€‚

ç¬¬äºŒé˜¶æ®µçš„æ„ŸçŸ¥æ™ºèƒ½ï¼Œå³è®©æœºå™¨æ‹¥æœ‰è§†è§‰ã€å¬è§‰ã€è§¦è§‰ç­‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è‡ªåŠ¨é©¾é©¶æ±½è½¦åšçš„å°±æ˜¯è¿™ä¸€æ–¹é¢çš„ç ”ç©¶ï¼Œä½¿æœºå™¨é€šè¿‡ä¼ æ„Ÿå™¨å¯¹å‘¨å›´çš„ç¯å¢ƒè¿›è¡Œæ„ŸçŸ¥å’Œå¤„ç†ï¼Œä»è€Œå®ç°è‡ªåŠ¨é©¾é©¶ã€‚æ„ŸçŸ¥æ™ºèƒ½æ–¹é¢çš„æŠ€æœ¯ç›®å‰å‘å±•æ¯”è¾ƒæˆç†Ÿçš„é¢†åŸŸæœ‰è¯­éŸ³è¯†åˆ«å’Œå›¾åƒè¯†åˆ«ï¼Œæ¯”å¦‚åšå®‰å…¨é¢†åŸŸäººè„¸è¯†åˆ«æŠ€æœ¯çš„ Face++ï¼Œä»¥æˆç†Ÿçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯æ·±è€•ç”µå•†ã€çŸ­è§†é¢‘ç­‰é¢†åŸŸçš„ Yi+ï¼Œèƒ½å¤Ÿå¯¹å¤šç§è¯­è¨€è¿›è¡Œå‡†ç¡®è¯†åˆ«ç¿»è¯‘çš„ç§‘å¤§è®¯é£ç­‰ã€‚

ç¬¬ä¸‰é˜¶æ®µçš„è®¤çŸ¥æ™ºèƒ½ä¸å‰é¢åœ¨äººå·¥æ™ºèƒ½çš„ 3 å¤§åˆ†æ”¯é‡Œæåˆ°çš„è®¤çŸ¥ AI ç±»ä¼¼ï¼Œå°±æ˜¯è®©æœºå™¨æ‹¥æœ‰è‡ªå·±çš„è®¤çŸ¥ï¼Œèƒ½ç†è§£ä¼šæ€è€ƒã€‚è®¤çŸ¥æ™ºèƒ½æ˜¯ç›®å‰æœºå™¨å’Œäººå·®è·æœ€å¤§çš„é¢†åŸŸï¼Œå› ä¸ºè¿™ä¸ä»…æ¶‰åŠé€»è¾‘å’ŒæŠ€æœ¯ï¼Œè¿˜æ¶‰åŠå¿ƒç†å­¦ã€å“²å­¦å’Œè¯­è¨€å­¦ç­‰å­¦ç§‘ã€‚

# çŸ¥è¯†é¢†åŸŸ

## Mathematics | æ•°å­¦åŸºç¡€

## Machine Learning | æœºå™¨å­¦ä¹ 

Maximum Objective Function

å¸¸è§æœºå™¨å­¦ä¹ çš„ä»»åŠ¡å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹ä¸ƒä¸ªæ­¥éª¤ï¼š

Data Collection

Data Preparation

Build Model

Train Model

Evaluation

Tune

Predict

## Deep Learning | æ·±åº¦å­¦ä¹ 

![image](https://user-images.githubusercontent.com/5803001/43595548-846b86e0-96af-11e8-951b-ae913482c19c.png)

ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å¾€å¾€éœ€è¦å¤§é‡çš„é¢†åŸŸçŸ¥è¯†ä¸è®¡ç®—æ—¶é—´ï¼Œè€Œæ·±åº¦å­¦ä¹ ä¸ºæˆ‘ä»¬å¸¦æ¥äº†æ— é™çµæ´»çš„å‡½æ•°ã€é€šç”¨çš„å‚æ•°æ‹Ÿåˆã€é«˜é€Ÿå¯æ‰©å±•ç­‰ç‰¹æ€§çš„ç®—æ³•ã€‚

Traditional statistical models do very well on structured data, i.e. tabular data, but have notoriously struggled with unstructured data like images, audio, and natural language. Neural networks that contain many layers of neurons embody the research that is popularly called Deep Learning. The key insight and property of deep neural networks that make them suitable for modeling unstructured data is that complex data, like images, generally have many layers of unique features that are composed to produce the data. As a classic example: images have edges which form the basis for textures, textures form the basis for simple objects, simple objects form the basis for more complex objects, and so on. In deep neural networks we aim to learn these many layers of composable features.

Traditional statistical models do very well on structured data, i.e. tabular data, but have notoriously struggled with unstructured data like images, audio, and natural language. Neural networks that contain many layers of neurons embody the research that is popularly called Deep Learning. The key insight and property of deep neural networks that make them suitable for modeling unstructured data is that complex data, like images, generally have many layers of unique features that are composed to produce the data. As a classic example: images have edges which form the basis for textures, textures form the basis for simple objects, simple objects form the basis for more complex objects, and so on. In deep neural networks we aim to learn these many layers of composable features.

![image](https://user-images.githubusercontent.com/5803001/43685359-4a84c7d4-98e4-11e8-8bce-7ef4cd2aa686.png)

## NLP | è‡ªç„¶è¯­è¨€å¤„ç†

## Computer Vision | è®¡ç®—æœºè§†è§‰

# Terminology | é€šç”¨æ¦‚å¿µ

## Function | å‡½æ•°

ğŸ’¡ Sigmod $\sigma$ ğŸ’¡

ç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶ä½œç”¨å°±æ˜¯å¼•å…¥éçº¿æ€§ã€‚å…·ä½“çš„éçº¿æ€§å½¢å¼ï¼Œåˆ™æœ‰å¤šç§é€‰æ‹©ã€‚sigmoid çš„ä¼˜ç‚¹åœ¨äºè¾“å‡ºèŒƒå›´æœ‰é™ï¼Œæ‰€ä»¥æ•°æ®åœ¨ä¼ é€’çš„è¿‡ç¨‹ä¸­ä¸å®¹æ˜“å‘æ•£ã€‚å½“ç„¶ä¹Ÿæœ‰ç›¸åº”çš„ç¼ºç‚¹ï¼Œå°±æ˜¯é¥±å’Œçš„æ—¶å€™æ¢¯åº¦å¤ªå°ã€‚sigmoid è¿˜æœ‰ä¸€ä¸ªä¼˜ç‚¹æ˜¯è¾“å‡ºèŒƒå›´ä¸º (0, 1)ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ä½œè¾“å‡ºå±‚ï¼Œè¾“å‡ºè¡¨ç¤ºæ¦‚ç‡ã€‚Sigmoid å‡½æ•°æ˜¯ä¸€ä¸ªåœ¨ç”Ÿç‰©å­¦ä¸­å¸¸è§çš„ S å‹çš„å‡½æ•°ï¼Œä¹Ÿç§°ä¸º S å½¢ç”Ÿé•¿æ›²çº¿ã€‚Sigmoid å‡½æ•°ç”±ä¸‹åˆ—å…¬å¼å®šä¹‰ï¼Œå…¶å¯¼æ•°å¯ä»¥èŠ‚çº¦è®¡ç®—æ—¶é—´:

$$
S(x) = \frac{1}{1+ e^{-x}} \\
S'(x)=S(x)[1-S(x)]
$$

Geoff Hinton covered exactly this topic in his coursera course on neural nets. The problem with sigmoids is that as you reach saturation (values get close to 1 or 0), the gradients vanish. This is detrimental to optimization speed. Softmax doesn't have this problem, and in fact if you combine softmax with a cross entropy error function the gradients are just (z-y), as they would be for a linear output with least squares error.

```py
# sigmoid function
def sigmoid(x, deriv=False):
    if(deriv==True):
        return x*(1-x)
    return 1/(1+np.exp(-x))
```

## Model | æ¨¡å‹

## Optimization | ä¼˜åŒ–

## Networks | ç½‘ç»œ

Gradient âˆ‡ (å¾®åˆ†ç®—ç¬¦)ï¼šæ¢¯åº¦

æ¢¯åº¦å³æ˜¯æŸä¸ªå‡½æ•°çš„åå¯¼æ•°ï¼Œå…¶å…è®¸è¾“å…¥å¤šä¸ªå‘é‡ç„¶åè¾“å‡ºå•ä¸ªå€¼ï¼ŒæŸä¸ªå…¸å‹çš„å‡½æ•°å³æ˜¯ç¥ç»ç½‘ç»œä¸­çš„æŸå¤±å‡½æ•°ã€‚æ¢¯åº¦ä¼šæ˜¾ç¤ºå‡ºéšç€å˜é‡è¾“å…¥çš„å¢åŠ è¾“å‡ºå€¼å¢åŠ çš„æ–¹å‘ï¼Œæ¢è¨€ä¹‹ï¼Œå¦‚æœæˆ‘ä»¬è¦é™ä½æŸå¤±å€¼åˆ™åæ¢¯åº¦é€†å‘å‰è¡Œå³å¯ã€‚

æ¢¯åº¦ä¸‹é™æ³• Gradient Descent æ˜¯ä¸€ç§å¸¸ç”¨çš„ä¸€é˜¶(first-order)ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¯æ±‚è§£æ— çº¦æŸä¼˜åŒ–é—®é¢˜æœ€ç®€å•ã€æœ€ç»å…¸çš„æ–¹æ³•ä¹‹ä¸€ã€‚è€ƒè™‘æ— çº¦æŸä¼˜åŒ–é—®é¢˜$min_xf(x)$ï¼Œå…¶ä¸­$f(x)$ä¸ºè¿ç»­å¯å¾®å‡½æ•°ã€‚å¦‚æœèƒ½æ„é€ å‡ºä¸€ä¸ªåºåˆ—$x^0,x^1,...,x^t$æ»¡è¶³ï¼š

$$
f(x^{t+1}) < f(x^t),t=0,1,2...
$$

åˆ™ä¸æ–­æ‰§è¡Œè¯¥è¿‡ç¨‹å³å¯ä»¥æ”¶æ•›åˆ°å±€éƒ¨æå°ç‚¹ã€‚è€Œæ ¹æ®æ³°å‹’å±•ç¤ºæˆ‘ä»¬å¯ä»¥çŸ¥é“:

$$
f(x+\Delta x) \simeq f(x) + \Delta x^T \nabla f(x)
$$

äºæ˜¯ï¼Œå¦‚æœè¦æ»¡è¶³ $f(x+\Delta x) < f(x)$ï¼Œå¯ä»¥é€‰æ‹©:

$$
\Delta x = -{step} \nabla f(x)
$$

å…¶ä¸­$step$æ˜¯ä¸€ä¸ªå°å¸¸æ•°ï¼Œè¡¨ç¤ºæ­¥é•¿ã€‚ä»¥æ±‚è§£ç›®æ ‡å‡½æ•°æœ€å°åŒ–ä¸ºä¾‹ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•å¯èƒ½å­˜åœ¨ä¸€ä¸‹å‡ ç§æƒ…å†µï¼š

- å½“ç›®æ ‡å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œå±€éƒ¨æå°ç‚¹å°±å¯¹åº”ç€å‡½æ•°å…¨å±€æœ€å°å€¼æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¿«é€Ÿçš„æ‰¾åˆ°æœ€ä¼˜è§£ï¼›
- å½“ç›®æ ‡å‡½æ•°å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€å°å€¼æ—¶ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚å› æ­¤éœ€è¦ä»å¤šä¸ªéšæœºçš„èµ·ç‚¹å¼€å§‹è§£çš„æœç´¢ã€‚
- å½“ç›®æ ‡å‡½æ•°ä¸å­˜åœ¨æœ€å°å€¼ç‚¹ï¼Œåˆ™å¯èƒ½é™·å…¥æ— é™å¾ªç¯ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

# Back Propagationï¼šåå‘ä¼ æ’­

ç®€ç§°ä¸º Back propï¼Œå³å°†å‰å‘ä¼ æ’­è¾“å…¥å€¼è®¡ç®—å¾—å‡ºçš„è¯¯å·®åå‘ä¼ é€’åˆ°è¾“å…¥å€¼ä¸­ï¼Œç»å¸¸ç”¨äºå¾®ç§¯åˆ†ä¸­çš„é“¾å¼è°ƒç”¨ã€‚

# Rectified Linear Units or ReLU

Sigmoid å‡½æ•°çš„è¾“å‡ºé—´éš”ä¸º`[0,1]`ï¼Œè€Œ ReLU çš„è¾“å‡ºèŒƒå›´ä¸º`[0,infinity]`ï¼Œæ¢è¨€ä¹‹ Sigmoid æ›´åˆé€‚ Logistic å›å½’è€Œ ReLU æ›´é€‚åˆäºè¡¨ç¤ºæ­£æ•°ã€‚æ·±åº¦å­¦ä¹ ä¸­ ReLU å¹¶ä¸ä¼šå—åˆ¶äºæ‰€è°“çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(Vanishing Gradient Problem)ã€‚

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/2/1-QYeGYddNRbrBJjkNxzw9FQ.png)

# Tanh

Tanh å‡½æ•°æœ‰åŠ©äºå°†ä½ çš„ç½‘ç»œæƒé‡æ§åˆ¶åœ¨`[-1,1]`ä¹‹é—´ï¼Œè€Œä¸”ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œè¶Šé è¿‘ 0 çš„åœ°æ–¹æ¢¯åº¦å€¼è¶Šå¤§ï¼Œå¹¶ä¸”æ¢¯åº¦çš„èŒƒå›´ä½äº`[0,1]`ä¹‹é—´ï¼Œå’Œ Sigmoid å‡½æ•°çš„èŒƒå›´ä¸€è‡´ï¼Œè¿™ä¸€ç‚¹ä¹Ÿèƒ½æœ‰åŠ©äºé¿å…æ¢¯åº¦åå·®ã€‚

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/2/1-K9g9EOeQ9Ca0jdOMmXKrQg.png)

# LSTM/GRU

æœ€æ—©è§äº Recurrent Neural Networksï¼Œä¸è¿‡åŒæ ·å¯ä»¥ç”¨äºå…¶ä»–å†…å­˜å•å…ƒè¾ƒå°‘çš„åœ°æ–¹ã€‚å…¶ä¸»è¦å¯ä»¥åœ¨è®­ç»ƒä¸­ä¿æŒè¾“å…¥çš„çŠ¶æ€ï¼Œä»è€Œé¿å…ä¹‹å‰å› ä¸º RNN ä¸¢å¤±è¾“å…¥å…ˆéªŒä¸Šä¸‹æ–‡è€Œå¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

# Softmax

Softmax å‡½æ•°å¸¸ç”¨äºç¥ç»ç½‘ç»œçš„æœ«ç«¯ä»¥æ·»åŠ åˆ†ç±»åŠŸèƒ½ï¼Œè¯¥å‡½æ•°ä¸»è¦æ˜¯è¿›è¡Œå¤šå…ƒé€»è¾‘æ–¯è’‚å›å½’ï¼Œä¹Ÿå°±å¯ä»¥ç”¨äºå¤šå…ƒåˆ†ç±»é—®é¢˜ã€‚é€šå¸¸ä¼šä½¿ç”¨äº¤å‰ç†µä½œä¸ºå…¶æŸå¤±å‡½æ•°ã€‚

# L1 & L2 Regularization

æ­£åˆ™åŒ–é¡¹é€šè¿‡å¯¹ç³»æ•°æ·»åŠ æƒ©ç½šé¡¹æ¥é¿å…è¿‡æ‹Ÿåˆï¼Œæ­£åˆ™åŒ–é¡¹ä¹Ÿèƒ½å¤ŸæŒ‡æ˜æ¨¡å‹å¤æ‚åº¦ã€‚L1 ä¸ L2 çš„åŒºåˆ«åœ¨äº L1 èƒ½å¤Ÿä¿è¯æ¨¡å‹çš„ç¨€ç–æ€§ã€‚å¼•å…¥æ­£åˆ™åŒ–é¡¹èƒ½å¤Ÿä¿è¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶ä¸”é¿å…åœ¨è®­ç»ƒæ•°æ®ä¸­è¿‡æ‹Ÿåˆã€‚

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/2/1-XkDC2Iwb9jSyRIWBUoDFtQ.png)

# Drop out

Drop out åŒæ ·å¯ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”èƒ½ä»¥è¿‘ä¼¼æŒ‡æ•°çš„æ—¶é—´æ¥åˆå¹¶å¤šä¸ªä¸åŒçš„ç¥ç»ç½‘ç»œç»“æ„ã€‚è¯¥æ–¹æ³•ä¼šéšæœºåœ°åœ¨æ¯ä¸€å±‚ä¸­é€‰æ‹©ä¸€äº›æ˜¾æ€§å±‚ä¸éšå±‚ï¼Œåœ¨æˆ‘ä»¬çš„å®è·µä¸­é€šå¸¸ä¼šç”±å›ºå®šæ¯”ä¾‹çš„å±‚ Drop out å†³å®šã€‚

# Batch Normalization

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¦‚æœæœ‰å¤ªå¤šçš„å±‚æ¬¡ä¼šå¯¼è‡´æ‰€è°“çš„ Internal Covariate Shiftï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­å› ä¸ºç½‘ç»œå‚æ•°çš„å˜åŒ–å¯¼è‡´ç½‘ç»œæ¿€æ´»åˆ†å¸ƒçš„å˜åŒ–ã€‚å¦‚æœæˆ‘ä»¬èƒ½å‡å°‘è¿™ç§å˜é‡è¿ç§»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¿«åœ°è®­ç»ƒç½‘ç»œã€‚Batch Normalization åˆ™é€šè¿‡å°†æ¯ä¸ªå¤„ç†å—è¿›è¡Œæ­£åˆ™åŒ–å¤„ç†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

# Objective Functions

ä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°æˆ–è€… Optimization Score Functionï¼ŒæŸä¸ªæ·±åº¦å­¦ä¹ ç½‘ç»œçš„ç›®æ ‡å³æ˜¯æœ€å°åŒ–è¯¥å‡½æ•°å€¼ä»è€Œæå‡ç½‘ç»œçš„å‡†ç¡®åº¦ã€‚

# F1/F Score

ç”¨äºè¡¡é‡æŸä¸ªæ¨¡å‹çš„å‡†ç¡®åº¦çš„æ ‡å‡†:

```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
Precision = True Positives / (True Positives + False Positives)
Recall = True Positives / (True Positives + False Negatives)
```

# Cross Entropy

ç”¨äºè®¡ç®—é¢„æµ‹æ ‡ç­¾å€¼ä¸çœŸå®æ ‡ç­¾å€¼ä¹‹é—´çš„å·®è·ï¼ŒåŸºæœ¬çš„å®šä¹‰å¦‚ä¸‹:
![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2016/12/2/1-9ZBskBY_piVwqC4GdZRl8g.png)
